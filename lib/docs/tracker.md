# EnigmaDB â€” Engineering Tracker

*Last updated: 16 Aug 2025 (IST)*

This is the living, endâ€‘toâ€‘end tracker for EnigmaDB: status, scope, design decisions, TODOs, experiments, and reading list. It is meant to be **authoritative** for dayâ€‘toâ€‘day execution.

---

## Legend

* **âœ… Done** â€“ implemented, tested, and merged
* **ğŸŸ¡ In Progress** â€“ partially implemented or under active work
* **ğŸ§ª Planned/Experiments** â€“ prototypes and perf studies to run
* **ğŸ”œ TODO** â€“ backlog, not started
* **âš ï¸ Risk/Open Q** â€“ needs decision or carries risk

---

## Project Snapshot

* **WAL**: âœ… Writers, rotation, periodic flush thread, `loadAll()`; codec OK. CRC util exists.
* **MemTable**: âœ… AVLâ€‘backed store; tombstones; freeze/unfreeze; size tracking; manager with rotation.
* **Varints & Byte I/O**: âœ… Robust varint32/64; byte parser utilities.
* **Types**: âœ… `Field` variant + `TypeRegistry` + (de)serialization; UUID, Binary, etc.
* **Compression**: âœ… LZ4 wrapper + tests. Noâ€‘op compressor available.
* **SSTable Bringâ€‘up**: ğŸŸ¡ KeyEncoder/BlockEncoder/FilterPolicy interfaces created; implementations TBD.
* **Flusher**: ğŸ”œ Wire MemTable â†’ SSTable writer thread; gate with WAL sync.
* **Compaction/Manifest/Query/Coordinator/Cluster/Networking**: ğŸ”œ Not implemented yet; scoped below.
* **Performance Roadmap**: ğŸ§ª io\_uring, mmap, SIMD listed as targeted experiments once storage path is correct.

> **Note on io\_uring:**  io\_uring for WAL appends and later SSTable flush/compaction I/O to reduce syscall overhead, leverage SQ/CQ batching, file registration, and fixed buffers. Weâ€™ll test it after correctness is locked in on the POSIX path.

---

## Architecture & Components â€” Deep Tracker

### 0) Engineering Foundations

**Purpose**: Tooling, quality bars, and common infra.

**State**: âœ… Catch2 harness; âœ… spdlog; âœ… CMake (Win/Linux); âœ… `.clang-format`; ğŸ§ª Sanitizers/Staticâ€‘analysis to add; ğŸ§ª Bench harness.

**TODO / Checklist**

* ğŸ”œ Add CI pipeline (build + tests + linters + ASan/UBSan/TSan matrix)
* ğŸ”œ Crossâ€‘platform file I/O abstraction (POSIX + Win32) with clean edges
* ğŸ”œ Fuzz tests (libFuzzer/AFL++) for WAL codec and varints
* ğŸ”œ Propertyâ€‘based tests (RapidCheck) for key/order laws
* ğŸ”œ Benchmark harness (Google Benchmark) for read/write paths
* ğŸ”œ Observability: metrics (Prometheus), tracing (OpenTelemetry), structured logs

**Experiments**

* ğŸ§ª Compare build flags (LTO, PGO) on hot loops (varints, encoders)
* ğŸ§ª Impact of allocator choices (tcmalloc/jemalloc vs system)

**Research**

* Build/CI patterns in RocksDB/ClickHouse; Fuzzing case studies in storage engines

---

### 1) Writeâ€‘Ahead Log (WAL)

**Purpose**: Durability for unflushed MemTables; crash recovery.

**State**: âœ… `WALWriter`, `WALManager`, rotation, periodic flush, `loadAll()`, codec; âœ… roundâ€‘trip tests; ğŸŸ¡ Integrate CRC into record format; ğŸ”œ richer corruption handling.

**Design Notes**

* Appendâ€‘only files with sizeâ€‘based rotation
* Status+fastâ€‘pointer style decoding in codec
* Background flush thread across writers
* Recovery scans rotated files sequentially

**TODO / Checklist**

* ğŸ”œ Integrate CRC32 into each record (header) and verify on read
* ğŸ”œ Corruption handling policy: skipâ€‘record vs stopâ€‘file vs quarantine
* ğŸ”œ Sync semantics: `fsync`/`fdatasync` policy and batching
* ğŸ”œ Rotation manifest: minimal index of segments for faster recovery
* ğŸ”œ Backpressure hooks when disks are slow (expose metrics)

**Experiments**

* ğŸ§ª `write`+`fdatasync` vs `pwrite`+`FUA` (where available)
* ğŸ§ª `mmap` vs buffered I/O for append (page cache behavior)
* ğŸ§ª **io\_uring**: SQPOLL, IOPOLL, fixed buffers, file registration; batch submits; compare QD
* ğŸ§ª Preallocate with `fallocate` / sparse files; measure fragmentation

**Research**

* WAL durability patterns (Postgres, RocksDB)
* Kernel I/O paths; io\_uring internals and caveats; fsync realities on ext4/xfs/ntfs

**Open Questions**

* âš ï¸ Exact record header layout (magic/version/len/crc/flags)
* âš ï¸ Crashâ€‘atomics: When to consider a record committed (write barrier ordering)

---

### 2) MemTable & Manager

**Purpose**: Inâ€‘memory, ordered writes; source of truth until flush.

**State**: âœ… AVL store; âœ… tombstones; âœ… freeze/unfreeze; âœ… size tracking; âœ… manager with autoâ€‘rotate & `flushOldestFrozen()`.

**Design Notes**

* HLC timestamps per entry for lastâ€‘writeâ€‘wins
* Composite keys via `Key{std::vector<Field>}`
* Type system with `TypeRegistry` to define ordering, (de)serialization

**TODO / Checklist**

* ğŸ”œ Concurrency properties: validate RCUâ€‘like or RWLock semantics under load
* ğŸ”œ Memory accounting & soft/hard limits with backpressure to writers
* ğŸ”œ Pluggable comparator per key schema (stable total order)
* ğŸ”œ TTL & tombstone expiration hooks (metadata on entries)

**Experiments**

* ğŸ§ª AVL vs Skipâ€‘list vs ART for write/read latency tradeâ€‘offs
* ğŸ§ª GC cadence for tombstones vs flush/compaction timing

**Research**

* MemTable structures in LevelDB/RocksDB; SSTable row ordering constraints

**Open Questions**

* âš ï¸ Comparator stability across versions (upgrades must not reorder)

---

### 3) Flusher (MemTable â†’ SSTable)

**Purpose**: Persist immutable MemTables as SSTables and advance durability frontier.

**State**: ğŸ”œ Not implemented; Manager exposes `flushOldestFrozen()` for handoff; encoders being designed.

**Design Notes (target)**

* Dedicated flusher thread(s) reading from a frozen queue
* Preâ€‘flush: ensure WAL is synced up to MemTable highâ€‘watermark
* Write SSTable with block encoder + index + filter + footer; update MANIFEST
* Publish new table atomically; remove WAL segments covered by snapshot

**TODO / Checklist**

* ğŸ”œ Implement Flusher thread; backpressure if queue grows
* ğŸ”œ WAL sync barrier integration and durable cutover
* ğŸ”œ Atomic publish (temp file â†’ rename) with fsync on directory
* ğŸ”œ Error handling & retry policy; partial file cleanup
* ğŸ”œ Hook metrics: flush size, duration, stall time, bytes written

**Experiments**

* ğŸ§ª Parallel flush (multiple frozen memtables) vs singleâ€‘threaded (seek contention)
* ğŸ§ª Large vs small memtable targets (impact on compaction/readâ€‘amp)

**Research**

* Flush scheduling in RocksDB (write stalls, level triggers)

**Open Questions**

* âš ï¸ Flush ordering guarantees (do we need strict FIFO?)

---

### 4) SSTable Format & Readers

**Purpose**: Immutable, sorted onâ€‘disk tables with efficient point/range reads.

**State**: ğŸŸ¡ Interfaces for KeyEncoder/BlockEncoder/FilterPolicy present; full implementation TBD.

**Design Notes (target)**

* **Data blocks**: prefixâ€‘compressed or varintâ€‘encoded entries
* **Index block**: sparse index (e.g., first key per data block)
* **Filter block**: bloom/Xor filter per block or per table
* **Footer**: fixedâ€‘size, magic/version, offsets/lengths for blocks
* **Table meta**: min/max key, sequence/timestamp range, checksum, compression

**TODO / Checklist**

* ğŸ”œ Finalize onâ€‘disk row layout (key encoding, value encoding, timestamp, tombstone)
* ğŸ”œ Implement BlockEncoder (restartâ€‘intervals / prefix compression)
* ğŸ”œ Implement perâ€‘block checksums (CRC32) and wholeâ€‘table checksum
* ğŸ”œ Implement IndexReader / TableReader, iterators (forward & reverse)
* ğŸ”œ Pluggable compression (Noop, LZ4; later Zstd)
* ğŸ”œ Bloom filter writer/reader; FPR configurable
* ğŸ”œ File format versioning & compatibility plan

**Experiments**

* ğŸ§ª Block size (4â€“64 KB) vs FPR vs latency
* ğŸ§ª Prefix vs dictionary encoding for strings
* ğŸ§ª LZ4 vs Zstd (ratio vs CPU) and compressibility by data type
* ğŸ§ª Checksumming strategies (perâ€‘block vs perâ€‘file)

**Research**

* LevelDB/RocksDB table formats; Cassandra SSTable evolution; Xor filters

**Open Questions**

* âš ï¸ Value format: column family vs wide rows vs raw blob first
* âš ï¸ Secondary indices (out of scope now) but reserve metadata

---

### 5) Manifest & Versioning

**Purpose**: Authoritative list of live SSTables and their metadata; enables crashâ€‘safe installs and rollbacks.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* MANIFEST journal recording table additions/removals with checksums
* Snapshot mechanism for quick bootstrap; atomic replace pattern
* Version numbers / epochs to coordinate with readers

**TODO / Checklist**

* ğŸ”œ Implement MANIFEST as an appendâ€‘only log + periodic snapshot
* ğŸ”œ On startup: replay MANIFEST â†’ build version state
* ğŸ”œ Directory fsync strategy (ensure metadata durability)

**Experiments**

* ğŸ§ª Snapshot cadence vs restart time vs MANIFEST growth

**Research**

* RocksDB VersionSet, MANIFEST patterns; ext4/xfs directory fsync behavior

**Open Questions**

* âš ï¸ Multiâ€‘CF support laterâ€”do we shard MANIFEST per keyspace?

---

### 6) Read Path / Query Layer (Single Node)

**Purpose**: `get`, `rangeScan`, `put`, `delete` API; merges MemTables + SSTables.

**State**: ğŸ”œ API scaffolding to define; relies on SSTable readers.

**Design Notes (target)**

* Merge views: Active MemTable, Frozen MemTables, SSTables
* Shortâ€‘circuit with bloom and sparse index; binary search within block
* Lastâ€‘writeâ€‘wins by timestamp; tombstones honored

**TODO / Checklist**

* ğŸ”œ Define public API; error model (Status)
* ğŸ”œ Implement MergingIterator over all sources
* ğŸ”œ RangeScan (inclusive/exclusive bounds, pagination)
* ğŸ”œ Batch operations; atomicity at row granularity

**Experiments**

* ğŸ§ª Microbenchmarks for point lookups under mixed workloads
* ğŸ§ª Effect of block cache on p50/p99 latency

**Research**

* Iterator design in LevelDB/RocksDB; Cassandra read repair basics

**Open Questions**

* âš ï¸ TTL semantics: read vs compaction enforcement

---

### 7) Caches (Block/Row/Key)

**Purpose**: Reduce I/O by caching decoded blocks/rows/keys.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Block cache (LRU/Clock/ARC); pinning for iterators
* Optional row cache for hot keys; key cache for index blocks

**TODO / Checklist**

* ğŸ”œ Implement block cache with sharded LRU
* ğŸ”œ Integrate with TableReader; expose metrics

**Experiments**

* ğŸ§ª Cache sizing vs hitâ€‘rate; shard count vs lock contention

**Research**

* Caching in RocksDB/Scylla; ARC vs LRU studies

**Open Questions**

* âš ï¸ Admission policies (TinyLFU?) and negative caching

---

### 8) Compaction Engine

**Purpose**: Reclaim space, reduce read amp, reconcile versions.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Tiered (leveled later): pick N overlapping tables â†’ merge â†’ new table(s)
* Respect tombstones and TTL; drop obsolete versions
* Throttled to avoid write stalls; IO scheduler aware

**TODO / Checklist**

* ğŸ”œ Compaction picker (sizeâ€‘tiered initially)
* ğŸ”œ Merge iterator respecting timestamps & deletes
* ğŸ”œ Throttle/slowdown logic; backpressure to writers

**Experiments**

* ğŸ§ª Sizeâ€‘tiered vs leveled on write amp/read amp/space amp
* ğŸ§ª Compaction parallelism vs foreground latency
* ğŸ§ª IO scheduling with **io\_uring** (batched reads/writes)

**Research**

* LevelDB (leveled), Cassandra (STCS/LCS/TWCS), RocksDB compaction filters

**Open Questions**

* âš ï¸ Timeâ€‘windowed compaction for timeâ€‘series use cases

---

### 9) Coordinator (Single Node â†’ Cluster Entry)

**Purpose**: Entry point for client requests; enforces consistency when clustered.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* For single node: passâ€‘through to local engine
* For cluster: route to replicas based on token ring; collect acks

**TODO / Checklist**

* ğŸ”œ Define Coordinator API (clientâ€‘facing)
* ğŸ”œ Local fastâ€‘path; hooks for future quorum logic

**Experiments**

* ğŸ§ª API overhead under high QPS; threadâ€‘pool sizing

**Research**

* Cassandra coordinator behavior; speculative reads

**Open Questions**

* âš ï¸ Backâ€‘pressure from storage to coordinator under overload

---

### 10) Token Ring & Partitioning

**Purpose**: Shard keys across cluster via consistent hashing & vnodes.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Hash partitioner (Murmur3/XXH3) â†’ 64/128b tokens
* Virtual nodes per physical node for balance
* Replication factor â†’ replica set per token range

**TODO / Checklist**

* ğŸ”œ Token assignment & ring metadata
* ğŸ”œ Simple rebalancing (add/remove node) plan

**Experiments**

* ğŸ§ª Skew under different vnode counts; hotâ€‘key mitigation

**Research**

* Cassandra partitioner & vnode design; jump consistent hash

**Open Questions**

* âš ï¸ Token movement protocols and streaming

---

### 11) Replication & Repair

**Purpose**: Durability & availability across nodes; eventual convergence.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Tunable consistency: ONE/QUORUM/ALL
* Hinted handoff when replicas are down
* Read repair and antiâ€‘entropy (Merkle trees later)

**TODO / Checklist**

* ğŸ”œ Write path fanâ€‘out and ack tracking
* ğŸ”œ Hinted handoff queue & replay
* ğŸ”œ Basic read repair on mismatches

**Experiments**

* ğŸ§ª Latency vs consistency level tradeâ€‘offs
* ğŸ§ª Impact of clock skew on read repair decisions (HLC)

**Research**

* Dynamoâ€‘style replication; Cassandra read repair & repair service

**Open Questions**

* âš ï¸ Conflict resolution beyond LWW (future: CRDTs?)

---

### 12) Gossip & Membership (Anti-Entropy)

**Purpose**: Discover peers, exchange liveness/state, drive ring changes.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Periodic heartbeats; phiâ€‘accrual failure detector
* Exchange token ring and schema digests

**TODO / Checklist**

* ğŸ”œ Gossip protocol skeleton (UDP/TCP)
* ğŸ”œ Failure detection thresholds & quarantine

**Experiments**

* ğŸ§ª Failure detector falseâ€‘positive rate tuning

**Research**

* Cassandra Gossip, SWIM protocols, phiâ€‘accrual detector

**Open Questions**

* âš ï¸ Secure gossip (auth/TLS) from day one?

---

### 13) Networking Layer

**Purpose**: RPC for client & interâ€‘node; efficient framing and backpressure.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Start with simple lengthâ€‘prefixed TCP framing; later gRPC
* Timeouts, retries, exponential backoff
* Flow control/backpressure; connection pooling

**TODO / Checklist**

* ğŸ”œ Define wire messages (Get/Put/Delete/Range, HintedHandoff, Gossip)
* ğŸ”œ Implement blocking TCP framing; codecs reuse varints
* ğŸ”œ Add TLS (later)

**Experiments**

* ğŸ§ª **io\_uring** for network send/recv (Linux) vs epoll/kqueue
* ğŸ§ª Nagle/Delayed ACK settings; keepâ€‘alive probing

**Research**

* RPC design in Scylla (Seastar) and gRPC internals

**Open Questions**

* âš ï¸ Custom RPC vs gRPC for longâ€‘term maintainability

---

### 14) Clocks & Timestamps

**Purpose**: Order updates and reconcile replicas.

**State**: âœ… HLC implemented & tested.

**Design Notes**

* Monotonicity per node; combine physical time with logical counter
* Use in write path and during compaction

**TODO / Checklist**

* ğŸ”œ Persist last HLC on shutdown to avoid monotonicity regressions
* ğŸ”œ Clock skew monitoring (metrics)

**Experiments**

* ğŸ§ª Simulate skew and bursty traffic; validate monotonicity

**Research**

* Literature on Hybrid Logical Clocks; Cassandra timestamp behavior

**Open Questions**

* âš ï¸ Crossâ€‘DC skew policies and clientâ€‘assigned timestamps

---

### 15) Schema, Data Model & Types

**Purpose**: Column families/keyspaces; types and serialization policy.

**State**: âœ… Core `Field`/`TypeRegistry` with ordering & (de)serialization.

**Design Notes**

* Composite primary keys; stable comparators
* Perâ€‘column metadata (TTL, default value) later

**TODO / Checklist**

* ğŸ”œ Formalize schema metadata; persist in MANIFEST or schema file
* ğŸ”œ Migration/versioning plan for types

**Experiments**

* ğŸ§ª Sorting stability under mixed types; stress with random generators

**Research**

* Cassandra schema (CQL), type evolution patterns

**Open Questions**

* âš ï¸ Handling of collations/locales for strings (initially bytewise)

---

### 16) Observability & Ops

**Purpose**: Make the system diagnosable and operable.

**State**: ğŸ”œ Minimal logging; metrics/tracing not yet integrated.

**Design Notes (target)**

* Metrics: counters, histograms for latencies, sizes, QPS
* Traces around write/read/flush/compact
* Health endpoints; debug dumps

**TODO / Checklist**

* ğŸ”œ Add metrics library integration (Prometheus)
* ğŸ”œ Span tracing via OpenTelemetry C++
* ğŸ”œ Verbose logging gates for hot paths

**Experiments**

* ğŸ§ª SLOs for p50/p95/p99 latencies; alerting thresholds

**Research**

* Observability in DB engines, RED/USE methodologies

**Open Questions**

* âš ï¸ Cardinality control for labels (donâ€™t explode metric space)

---

### 17) Backup/Restore & Snapshots (Future)

**Purpose**: Disaster recovery and portability.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* Filesystem snapshots or tableâ€‘copy with manifests
* Incremental backup via MANIFEST delta

**TODO / Checklist**

* ğŸ”œ Consistent snapshot cut (pause flush/compaction briefly)

**Experiments**

* ğŸ§ª Backup throughput and restore times

**Research**

* Cassandra snapshots/hardlinks; RocksDB checkpoints

---

### 18) Security (Future)

**Purpose**: Basic authn/z and transport security.

**State**: ğŸ”œ Not implemented.

**Design Notes (target)**

* TLS for client & interâ€‘node
* Optional auth; tokenâ€‘based or mTLS identities

**TODO / Checklist**

* ğŸ”œ Pluggable auth interceptor

**Open Questions**

* âš ï¸ Multiâ€‘tenant isolation (explicitly outâ€‘ofâ€‘scope for v1)

---

## Crossâ€‘Cutting Invariants

* **Durability**: Acknowledged write implies WAL persisted to durability policy
* **Ordering**: Stable key comparator; HLC total order within shard
* **Immutability**: SSTables are appendâ€‘once then readâ€‘only
* **Atomic publish**: New table visible only after full write + fsync + rename
* **Crash safety**: MANIFEST is single source of truth at startup
* **Backpressure**: Writers must slow when flush/compaction lags

---

## Roadmap & Gating Criteria

### Milestone A â€” Complete Singleâ€‘Node Storage Engine (MVP)

* [ ] WAL CRC integrated; corruption policy documented
* [ ] Flusher thread; WAL sync barrier; atomic SSTable publish
* [ ] SSTable v1: data/index/filter/footer + readers + iterators
* [ ] Read path merging MemTable/Frozen/SSTables
* [ ] MANIFEST v1 with replay on start
* **Gate**: Endâ€‘toâ€‘end tests pass; crashâ€‘recovery replay verified; p95 latency targets set

### Milestone B â€” Performance Pass

* [ ] Block cache (LRU) with metrics
* [ ] Bench harness & baseline results
* [ ] **io\_uring** experiments for WAL and flush path (Linux)
* [ ] Compression tuning (LZ4/Zstd) & block size tuning
* **Gate**: Benchmarks show material improvement over baseline

### Milestone C â€” Cluster Bringâ€‘up (Alpha)

* [ ] Token ring + vnode assignment
* [ ] Simple gossip + failure detector
* [ ] Replication (RF=N) with ONE/QUORUM/ALL
* [ ] Coordinator routing + hinted handoff
* **Gate**: 3â€‘node cluster stable under Jepsenâ€‘like basic tests (no partitions at first)

### Milestone D â€” Consistency & Repair (Beta)

* [ ] Read repair & basic antiâ€‘entropy
* [ ] Timeâ€‘windowed compaction option
* [ ] Observability (metrics/tracing dashboards)
* **Gate**: Convergence after transient failures verified; chaos tests green

---

## Experiments Matrix (Quick Reference)

| Area               | Hypothesis                                  | Method                     | Metric                        |
| ------------------ | ------------------------------------------- | -------------------------- | ----------------------------- |
| WAL fsync batching | Batch improves throughput                   | Vary group commit sizes    | ops/s, p99                    |
| io\_uring vs write | Fewer syscalls â†’ lower tail                 | Submit batches with SQPOLL | p95/p99 latency               |
| Block size         | Larger blocks reduce reads but increase CPU | Sweep 4â€“64KB               | Read amp, CPU%                |
| Bloom FPR          | Lower FPR reduces disk hits                 | Sweep 0.1â€“10%              | False positives, read latency |
| Cache shard count  | More shards reduce lock contention          | Sweep 1â€“64                 | Throughput, mutex wait        |
| Compaction type    | Tiered vs leveled tradeâ€‘offs                | Replay workload traces     | Write/read/space amp          |

---

## Reading & Research List (by Component)

* **WAL & Durability**: Postgres WAL internals; fsync semantics; storage write barriers
* **SSTable/LSM**: LevelDB & RocksDB table format & compaction papers; Cassandra SSTable docs
* **Filters**: Bloom filters; blocked bloom; Xor filters
* **Consistency**: Dynamo; Cassandra consistency levels; HLC literature
* **Gossip/Failure**: SWIM; phiâ€‘accrual failure detector
* **Networking**: gRPC internals; TCP performance tuning guides
* **Perf**: io\_uring guides; CPU cache/NUMA; SIMD intro for parsing
* **Testing**: Jepsen case studies; propertyâ€‘based testing in C++

---

## Open Risks & Decisions Log

* **Record header**: finalize WAL & SSTable checksum/flags layout (owner: storage)
* **Comparator stability**: define wireâ€‘compatible sort order (owner: types)
* **RPC choice**: custom vs gRPC (owner: networking)
* **Cache policy**: LRU vs ARC/TinyLFU (owner: performance)

---

## Action Items

1. Implement WAL CRC + corruption policy â†’ update tests
2. Finish KeyEncoder/BlockEncoder; define SSTable row format (doc + code)
3. Implement Flusher thread with WAL sync barrier and atomic publish
4. Minimal TableReader + MergingIterator; singleâ€‘node `get/put/delete`
5. Write endâ€‘toâ€‘end integration test: write N, crash, recover, verify

---

Maintained by: frostzt
